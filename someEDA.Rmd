---
title: "Some EDA"
output: html_document
---

```{r}
setwd("C:/Users/Nox/Desktop/NU Classes/Data Mining/Kaggle/kagglerepo/Future-Purchases-from-Book-Seller")

library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(car)
```

```{r}
maindata = read.csv("merged_train_adv.csv")

orders = read.csv("orders.csv")

ajitdata = read.csv("ajit.csv")
advdata = read.csv("adv.csv")
```

```{r}
summary(maindata)
```


```{r}
cor(maindata)
```

```{r}
plot(maindata)
```


```{r}
plot(maindata$tot_price, maindata$logtarg)
boxplot(maindata$tot_price, maindata$logtarg)
```

```{r}
only_zero_price = orders[orders$price == 0,]
summary(only_zero_price)

orders[orders$category == 99]

```
```{r}
summary(orders)
```


```{r}
n_distinct(only_zero_price$category)
only_zero_price
histogram(only_zero_price$category)
```


```{r}
qplot(data = maindata, x = logtarg)
```

```{r}
ajitdata
advdata
check_GO = merge(ajitdata, advdata, by="id")

write.csv(check_GO, "check_GO.csv")
```
```{r}
ajit_preds = read.csv("ajit_training_predictions.csv")
check_GO2 = merge(ajit_preds, advdata, by="id")
write.csv(check_GO2, "check_GO2.csv")
```


```{r}
bwd_check = read.csv("bwd.csv")
xgb_check = read.csv("xgb_submission.csv")
xgb2_check = read.csv("xgb_submission_2.csv")
ajit_check = read.csv("ajit.csv")
adv_check = read.csv("adv.csv")
```


```{r}
hist(bwd_check$yhat,breaks = 50)
hist(xgb_check$yhat,breaks = 50)
hist(xgb2_check$yhat,breaks = 50)
hist(ajit_check$yhat,breaks = 50)
hist(adv_check$yhat,breaks = 50)
```


```{r}
library(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test


bstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")

bstDense <- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nthread = 2, nround = 2, objective = "binary:logistic")

dtrain <- xgb.DMatrix(data = train$data, label=train$label)
dtest <- xgb.DMatrix(data = test$data, label=test$label)

watchlist <- list(train=dtrain, test=dtest)

bst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nround=2, watchlist=watchlist, objective = "binary:logistic")

bst <- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nround=2, watchlist=watchlist, eval.metric = "error", eval.metric = "logloss", objective = "binary:logistic")
```

```{r}
pred <- predict(bst, test$data)

prediction <- as.numeric(pred > 0.5)
err <- mean(as.numeric(pred > 0.5) != test$label)
print(paste("test-error=", err))

```


```{r}
param <- list(objective = "binary:logistic",
          eval_metric = "auc",
          max_depth = 5,
          eta = 0.1,
          gamma = 0.1, 
          subsample = 0.8,
          colsample_bytree = 0.8, 
          min_child_weight = 1,
          max_delta_step = 5,
          scale_pos_weight = 1
          )
```

```{r}
library(xgboost)
train <- read.csv("merged_train_adv_new.csv")
test <- read.csv("test_adv_new.csv")

frm <- as.formula("logtarg ~ slstyr + last_pur + slslyr + ordbfr + ordtyr + tot_orders + 
    first_pur + tot_cat + activity + ordlyr + last_pur:slslyr + 
           slstyr:ordtyr + ordbfr:tot_cat + slstyr:tot_cat + ordtyr:activity + 
           ordtyr:first_pur + tot_cat:ordlyr + tot_orders:activity + 
           slstyr:activity - activity - slstyr - last_pur - ordlyr")

dftrain = model.matrix(frm, data = train)

#dftrain = as.matrix(train[,-c(1,20,21,22)])
dftest = as.matrix(test[,-1])
logtarg_convert = ifelse((train$logtarg > 0), 1, 0)

dtrain <- xgb.DMatrix(data = dftrain, label=as.matrix(logtarg_convert))
dtest <- xgb.DMatrix(data = dftest)

watchlist <- list(train=dtrain)
bst <- xgb.train(data=dtrain, params = param, nthread = 7, nround=1000, watchlist=watchlist)
```

```{r}
pred <- predict(bst, dtrain)

prediction <- as.numeric(pred > 0.5)
err <- mean(as.numeric(pred > 0.5) != logtarg_convert)
print(paste("test-error=", err))
```

```{r}
xgb.dump(bst, with_stats = T)

```

```{r}
xgb.save(bst, "xgboost.model")
pred_test = predict(bst, dtest)
#df_predtest = as.data.frame(pred_test)
df_output = data.frame(id = test$id, yhat = pred_test)
```
```{r}
write.csv(df_output, "xgb_log_test.csv", row.names = F)
```


```{r}
df_output_train = data.frame(train, yhat = pred, output = prediction)
write.csv(df_output_train, "xgb_log_train.csv", row.names = F)
```

```{r}
set.seed(2018-1-20)

param <- list(objective = "binary:logistic",
          eval_metric = "auc",
          max_depth = 5,
          eta = 0.1,
          gamma = 0.1, 
          subsample = 0.8,
          colsample_bytree = 0.8, 
          min_child_weight = 1,
          max_delta_step = 5,
          scale_pos_weight = 1
          )
cv.nround = 1000
cv.nfold = 5

mdcv <- xgb.cv(data=dtrain, params = param, nthread=7, 
                    nfold=cv.nfold, nrounds=cv.nround,
                    verbose = T, early_stopping_rounds=50, maximize=TRUE)

#data=dtrain, max.depth=100, eta=1, nthread = 100, nround=100, watchlist=watchlist, eval.metric = "error", eval.metric = "logloss", eval.metric = "auc", objective = "binary:logistic"

```

```{r}
model_xgbcv = mdcv$best_iteration
pred2 <- predict(model_xgbcv, dtrain)

prediction2 <- as.numeric(pred > 0.5)
err <- mean(as.numeric(pred > 0.5) != logtarg_convert)
print(paste("test-error=", err))

df_output_train2 = data.frame(train, yhat = pred2, output = prediction2)
write.csv(df_output_train2, "xgb_log_train_cv.csv", row.names = F)
```


```{r}
best_param = list()
best_seednumber = 1234
best_logloss = Inf
best_logloss_index = 0

for (iter in 1:100) {
    param <- list(objective = "multi:softprob",
          eval_metric = "mlogloss",
          num_class = 12,
          max_depth = sample(6:10, 1),
          eta = runif(1, .01, .3),
          gamma = runif(1, 0.0, 0.2), 
          subsample = runif(1, .6, .9),
          colsample_bytree = runif(1, .5, .8), 
          min_child_weight = sample(1:40, 1),
          max_delta_step = sample(1:10, 1)
          )
    cv.nround = 1000
    cv.nfold = 5
    seed.number = sample.int(10000, 1)[[1]]
    set.seed(seed.number)
    mdcv <- xgb.cv(data=dtrain, params = param, nthread=6, 
                    nfold=cv.nfold, nrounds=cv.nround,
                    verbose = T, early.stop.round=8, maximize=FALSE)

    min_logloss = min(mdcv[, test.mlogloss.mean])
    min_logloss_index = which.min(mdcv[, test.mlogloss.mean])

    if (min_logloss < best_logloss) {
        best_logloss = min_logloss
        best_logloss_index = min_logloss_index
        best_seednumber = seed.number
        best_param = param
    }
}

nround = best_logloss_index
set.seed(best_seednumber)
md <- xgb.train(data=dtrain, params=best_param, nrounds=nround, nthread=6)
```

